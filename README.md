---

# ğŸ¥ LViT - Language meets Vision Transformer for Medical Image Segmentation

**LViT** is an advanced **AI-powered medical image segmentation system** that combines **Vision Transformers (ViT)** with **text annotations** to improve segmentation accuracy, especially in semi-supervised learning scenarios. This project leverages **multimodal learning** (image + text) to compensate for limited labeled medical data.

***

## ğŸš€ Project Overview

Medical image segmentation is critical for diagnosis and treatment planning, but obtaining high-quality labeled data is expensive and time-consuming. **LViT** addresses this challenge by:

- **Integrating text annotations** (doctor's reports) with medical images (X-rays, CT scans)
- **Using a hybrid CNN-ViT architecture** for local and global feature extraction
- **Supporting semi-supervised learning** with pseudo-label refinement (EPI mechanism)
- **Achieving state-of-the-art performance** on COVID-19 lung segmentation datasets

The goal is to make **medical image segmentation faster, more accurate, and more accessible**, even with limited labeled data.

***

## ğŸ§© System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INTERFACE                           â”‚
â”‚              (React / Streamlit Web App)                    â”‚
â”‚                                                             â”‚
â”‚   ğŸ“¤ Upload X-ray/CT Image  |  ğŸ“ Enter Medical Report     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ HTTP Request (Image + Text)
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BACKEND API SERVER                        â”‚
â”‚                  (Flask / FastAPI)                          â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Preprocessing Pipeline                             |    |
â”‚  â”‚  â€¢ Resize image to 224Ã—224                           â”‚   â”‚
â”‚  â”‚  â€¢ Normalize pixel values                            â”‚   â”‚
â”‚  â”‚  â€¢ Tokenize text with BERT                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ Preprocessed Data
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LViT MODEL                               â”‚
â”‚            (Hybrid CNN + Vision Transformer)                â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   CNN Branch     â”‚         â”‚   ViT Branch     â”‚         â”‚
â”‚  â”‚  (Local Details) â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ (Global Context) â”‚         â”‚
â”‚  â”‚                  â”‚         â”‚  + Text Fusion   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â”‚                            â”‚                   â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                     â”‚                                      â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚           â”‚  PLAM Module      â”‚  â† Merges features         â”‚
â”‚           â”‚ (Attention Fusion)â”‚                            â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                     â”‚                                      â”‚
â”‚                     â–¼                                      â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚           â”‚ Segmentation Maskâ”‚                             â”‚
â”‚           â”‚    (Output)      â”‚                             â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”‚ Prediction Result
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VISUALIZATION & REPORTING                      |
â”‚                                                             â”‚
â”‚  ğŸ“Š Mask Overlay on Image                                   â”‚
â”‚  â€¢ Original image with predicted mask                       â”‚
â”‚  â€¢ Color-coded infected regions                             â”‚
â”‚  â€¢ Side-by-side comparison view                             â”‚
â”‚                                                             â”‚
â”‚  ğŸ“„ Clinical Report Generation                              | 
â”‚  â€¢ Save results as PDF/PNG                                  â”‚
â”‚  â€¢ Doctor notes & recommendations                           â”‚
â”‚  â€¢ Downloadable segmentation masks                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

***

## âš™ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| **Frontend** | React + TypeScript / Streamlit |
| **Backend** | Flask / FastAPI (Python) |
| **Models** | PyTorch (LViT - CNN + ViT Hybrid) |
| **Text Processing** | HuggingFace Transformers (BERT) |
| **Data** | COVID-19 X-ray & CT datasets (QaTa-COV19, MosMedData, ESO-CT) |
| **Model Hosting** | Hugging Face Hub (optional) |
| **Environment** | Google Colab / Jupyter Notebook / Local Setup |
| **Version Control** | Git & GitHub |

***

## âœ¨ Key Features

âœ… **Multimodal Learning** â€” Combines medical images with text annotations (doctor reports)  
âœ… **Hybrid Architecture** â€” Double-U structure with CNN (local features) + ViT (global context)  
âœ… **Semi-Supervised Learning** â€” Exponential Pseudo-label Iteration (EPI) for unlabeled data  
âœ… **State-of-the-Art Performance** â€” 83.66% Dice score on QaTa-COV19, 74.57% on MosMedData  
âœ… **Text-Guided Segmentation** â€” Uses clinical reports to improve prediction accuracy  
âœ… **Real-time Inference** â€” Web interface for uploading images and viewing results  
âœ… **Performance Metrics** â€” Dice score, mIoU, visual overlays, and confidence scores  

***

## ğŸ“Š Datasets

This project uses three medical image segmentation datasets with text annotations:

| Dataset | Type | Images | Description |
|---------|------|--------|-------------|
| **QaTa-COV19** | X-ray | 9,258 | COVID-19 lung infection with manual annotations |
| **MosMedData** | CT | 2,729 | Lung infection CT scans |
| **ESO-CT** | CT | 286 | Esophageal cancer segmentation |



***

## ğŸ—ï¸ Model Architecture

### Double-U Structure

**LViT** uses a hybrid CNN-ViT architecture:

1. **U-Shaped CNN Branch**
   - Extracts local features (edges, boundaries)
   - Acts as segmentation head for final prediction

2. **U-Shaped ViT Branch**
   - Processes global context and cross-modal fusion
   - Integrates BERT-embedded text features with image patches

3. **Pixel-Level Attention Module (PLAM)**
   - Placed at skip connections
   - Merges CNN and ViT features for better boundary detection

4. **Exponential Pseudo-label Iteration (EPI)**
   - Refines pseudo-labels for unlabeled data
   - Uses Exponential Moving Average (EMA) for stability

5. **Language-Vision (LV) Loss**
   - Supervises unlabeled data using text-image similarity
   - Prevents mis-segmentation in semi-supervised learning

***
## ğŸ“‚ Datasets Used

This project uses several datasets:

- **QaTa-COV19 Dataset** â€” COVID-19 chest X-ray images with infection segmentation masks.  
  [Kaggle Dataset Link (Original)](https://www.kaggle.com/datasets/aysendegerli/qatacov19-dataset)

- **MosMedData+ Dataset** â€” COVID-19 chest CT scans with lesion and infection segmentation.  
  - Official Website: [medicalsegmentation.com COVID-19 dataset](http://medicalsegmentation.com/covid19/)  
  - Kaggle Link: [MosMedData+ on Kaggle](https://www.kaggle.com/datasets/maedemaftouni/covid19-ct-scan-lesion-segmentation-dataset)

- **MoNuSeG Dataset** (Demo) â€” Multi-organ nucleus segmentation dataset from histopathology images.  
  [MoNuSeG Official Challenge Site](https://monuseg.grand-challenge.org/Data/)

- **ESO-CT Dataset** â€” CT scans with esophageal cancer segmentation.  
  Dataset references: [1] [2] (Add URLs or citation here)

***

## ğŸ“ˆ Performance Results

### Fully-Supervised Learning

| Model | Dice (QaTa-COV19) | mIoU | Dice (MosMedData) | mIoU |
|-------|-------------------|------|-------------------|------|
| U-Net | 79.02 | 69.46 | 64.60 | 50.73 |
| nnU-Net | 80.42 | 70.81 | 72.59 | 60.36 |
| TransUNet | 78.63 | 69.13 | 71.24 | 58.44 |
| **LViT-T** | **83.66** | **75.11** | **74.57** | **61.33** |

### Semi-Supervised Learning (25% labels)

| Model | Dice (QaTa-COV19) | mIoU |
|-------|-------------------|------|
| DTC | 76.07 | 66.04 |
| MC-Net | 76.93 | 67.02 |
| **LViT-T (1/4)** | **80.95** | **71.31** |

***
